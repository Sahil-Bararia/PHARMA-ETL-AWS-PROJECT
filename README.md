# Pharma Sales Analytics Pipeline â€“ AWS ETL Project

## ðŸš€ Project Overview
This project demonstrates a complete **ETL and analytics pipeline for pharmaceutical sales data** using AWS services. Built using the **Medallion Architecture (Bronze, Silver, Gold layers)**, it replicates real-world consulting use cases often encountered in healthcare and pharma analytics.

---

## ðŸ—ï¸ Architecture Layer

- **Bronze Layer**: Raw CSV files ingested into S3.
- **Silver Layer**: Cleaned, deduplicated Parquet files written to S3.
- **Gold Layer**: Aggregated and analytics-ready data queried with Athena.

---

## ðŸ”§ AWS Services Used
- **S3** â€“ Data lake storage for raw, processed, and final datasets
- **AWS Glue** â€“ ETL jobs using PySpark (extract, transform, and load operations)
- **Athena** â€“ SQL-based analytics on curated data
- **AWS IAM** â€“ Role-based access and job permissions
- *(Optionally extensible to QuickSight or Redshift)*

---

## ðŸ“‚ Data Layers (Medallion Style)
- `zs-pharma-etl/raw/` â€“ CSV files as-is from source
- `zs-pharma-etl/processed/` â€“ Cleaned Parquet format
- `zs-pharma-etl/analytics/` â€“ Final transformed outputs

---

## ðŸ” Sample Queries (Athena)
```sql
-- LAG example: Compare monthly sales with previous month
SELECT product, year, month, 
       sales, 
       LAG(sales) OVER (PARTITION BY product ORDER BY year, month) AS previous_sales
FROM analytics_table;

-- LEAD example: Forecast trend with next month sales
SELECT product, year, month, 
       sales, 
       LEAD(sales) OVER (PARTITION BY product ORDER BY year, month) AS next_sales
FROM analytics_table;

-- YOY Growth
SELECT product, year, SUM(sales) AS total_sales
FROM analytics_table
GROUP BY product, year;
```

---

## âœ… Features
- Handles large pharma datasets (sales, regions, SKUs)
- Converts CSV to columnar Parquet format
- Performance-optimized querying with Athena
- Modular and extensible for future ML/BI integration

---

## ðŸ“Œ How to Run This Project
1. Upload sample `sales_data.csv` to S3 in the `raw/` folder
2. Create and run Glue Job (PySpark script)
3. Validate outputs in `processed/` and `analytics/` folders
4. Use Athena to run analytics queries on curated tables

---

## ðŸ§  Why Medallion Architecture?
The layered approach ensures:
- Data quality at every stage
- Simplified debugging
- Reusability across downstream systems (BI, ML)

---

## ðŸ‘¨â€ðŸ’» Author
**Sahil Bararia**  
Data Engineer | Power BI | Azure | AWS | Python

ðŸ”— [LinkedIn](https://www.linkedin.com/in/sahil-bararia-a09772232/)  
ðŸ”— [GitHub](https://github.com/Sahil-Bararia)

---

> Feel free to fork this repo and adapt it for your own data engineering interviews or consulting use cases.
